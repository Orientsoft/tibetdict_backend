from passlib.context import CryptContext
import bcrypt
from botok import Text
import hashlib

pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")


def verify_password(plain_password, hashed_password):
    return pwd_context.verify(plain_password, hashed_password)


def generate_salt():
    return bcrypt.gensalt().decode()


def get_password_hash(password):
    return pwd_context.hash(password)


def contenttomd5(origin: bytes):
    md5 = hashlib.md5()
    md5.update(origin)
    return md5.hexdigest()


def tokenize_words(content: str):
    a = ''
    try:
        t = Text(content)
        a = t.tokenize_words_raw_lines
        a = a.replace('_', '')
    except:
        pass
    finally:
        return a

def tokenize_sentence(para:str):
    import re
    para = re.sub(r"།(\s*)།", r"།།\n", para) # 去掉双垂线中间的空格
    para = re.sub('([།])([^།\\n])', r"\1\n\2", para)  # 单垂线后加换行符。若单垂线后为垂线或\n 则排除
    para = para.rstrip()  # 段尾如果有多余的\n就去掉它
    return para.split("\n")


if __name__ == '__main__':
    # origin = 'p1 ༄ ༅ །  འདུལ་བ་ ཀ་བཞུགས་ སོ །  p2 ln1 ༄ ༅ ༅ །  རྒྱ་གར་ སྐད་ དུ །  བི་ན་ཡ་བསྟུ །  བོད་ སྐད་ དུ །  འདུལ་བ་གཞི །  བམ་པོ་ དང་པོ །  དཀོན་མཆོག་ གསུམ་ ལ་ ཕྱག་ འཚལ་ ལོ །  གང་ གིས་ འཆིང་ ln2 རྣམས་ ཡང་དག་རབ་བཅད་ ཅིང་ །  མུ་སྟེགས་ཚོགས་ རྣམས་ ཐམས་ཅད་ རབ་ བཅོམ་ སྟེ །  སྡེ་དང་ བཅས་པ འི་ བདུད་ རྣམས་ ངེས་བཅོམ་ ནས །  བྱང་ཆུབ་ འདི་ བརྙེས་ དེ་ ལ་ ln3 ཕྱག་ འཚལ་ ལོ །  ཁྱིམ་དོན་ ཆེ་ཆུང་ སྤངས་ ཏེ་ དང་པོ ར་ རབ་ འབྱུང་ དཀའ །  རབ་བྱུང་ ཐོབ་ ནས་ ཡུལ་སྤྱད་ དག་ གིས་ དགའ་ ཐོབ་ དཀའ །  མངོན་ དགའ་ ཇི་ བཞིན་ དོན་ བསྐྱེད་ ཡང་ ln4 དག་ བྱེད་པ་ དཀའ །  ངུར་སྨྲིག་ གོས་ འཆང་མཁས་པ་ ཚུལ་ལས་ ཉམས་པ་ དཀའ །  གཞི་ རྣམས་ ཀྱི་ སྤྱི་སྡོམ་ ལ །  རབ་ འབྱུང་ གསོ་སྦྱོང་ གཞི་ དང་ ནི །  དགག་དབྱེ་ དབྱར་ དང་ ཀོ་ ln5 ལྤགས་གཞི །  སྨན་ དང་ གོས་ དང་ སྲ་བརྐྱང་ དང་ །  ཀཽ་ཤཱམ་བཱི་ དང་ ལས་ ཀྱི་ གཞི །  དམར་སེར་ཅན་ དང་ གང་ཟག་ དང་ །  སྤོ་ དང་ གསོ་སྦྱོང་ བཞག་པ་ དང་ །  གནས་ མལ་ དང་ ནི་ p3 ln1 རྩོད་པ་ དང་ །  དགེ་འདུན་ དབྱེན་ རྣམས་ བསྡུས་པ་ ཡིན །  རབ་ ཏུ་ འབྱུང་བ འི་ གཞི འི་ སྤྱི་ སྡོམ་ ལ །  ཤཱ་རི འི་ བུ་ དང་ མུ་སྟེགས་ཅན །  དགེ་ཚུལ་ གཉིས་ དང་ བྱ་རོག་ སྐྲོད །  དགྲ་བཅོམ་བསད་ དང་ ལག་རྡུམ་ གྱི །'
    # in_str = 'p1 ༄ ༅ །  འདུལ་བ་ ཀ་བཞུགས་ སོ །  p2 ln1 ༄ ༅ ༅ །  རྒྱ་གར་ སྐད་ དུ །  བི་ན་ཡ་བསྟུ །  བོད་ སྐད་ དུ །  འདུལ་བ་གཞི །  བམ་པོ་ དང་པོ །  དཀོན་མཆོག་ གསུམ་ ལ་ ཕྱག་ འཚལ་ ལོ །  གང་ གིས་ འཆིང་ ln2 རྣམས་ ཡང་དག་རབ་བཅད་ ཅིང་ །  མུ་སྟེགས་ཚོགས་ རྣམས་ ཐམས་ཅད་ རབ་ བཅོམ་ སྟེ །  སྡེ་དང་ བཅས་པ འི་ བདུད་ རྣམས་ ངེས་བཅོམ་ ནས །  བྱང་ཆུབ་ འདི་ བརྙེས་ དེ་ ལ་ ln3 ཕྱག་ འཚལ་ ལོ །  ཁྱིམ་དོན་ ཆེ་ཆུང་ སྤངས་ ཏེ་ དང་པོ ར་ རབ་ འབྱུང་ དཀའ །  རབ་བྱུང་ ཐོབ་ ནས་ ཡུལ་སྤྱད་ དག་ གིས་ དགའ་ ཐོབ་ དཀའ །  མངོན་ དགའ་ ཇི་ བཞིན་ དོན་ བསྐྱེད་ ཡང་ ln4 དག་ བྱེད་པ་ དཀའ །  ངུར་སྨྲིག་ གོས་ འཆང་མཁས་པ་ ཚུལ་ལས་ ཉམས་པ་ དཀའ །  གཞི་ རྣམས་ ཀྱི་ སྤྱི་སྡོམ་ ལ །  རབ་ འབྱུང་ གསོ་སྦྱོང་ གཞི་ དང་ ནི །  དགག་དབྱེ་ དབྱར་ དང་ ཀོ་ ln5 ལྤགས་གཞི །  སྨན་ དང་ གོས་ དང་ སྲ་བརྐྱང་ དང་ །  ཀཽ་ཤཱམ་བཱི་ དང་ ལས་ ཀྱི་ གཞི །  དམར་སེར་ཅན་ དང་ གང་ཟག་ དང་ །  སྤོ་ དང་ གསོ་སྦྱོང་ བཞག་པ་ དང་ །  གནས་ མལ་ དང་ ནི་ p3 ln1 རྩོད་པ་ དང་ །  དགེ་འདུན་ དབྱེན་ རྣམས་ བསྡུས་པ་ ཡིན །  རབ་ ཏུ་ འབྱུང་བ འི་ གཞི འི་ སྤྱི་ སྡོམ་ ལ །  ཤཱ་རི འི་ བུ་ དང་ མུ་སྟེགས་ཅན །  དགེ་ཚུལ་ གཉིས་ དང་ བྱ་རོག་ སྐྲོད །  དགྲ་བཅོམ་བསད་ དང་ ལག་རྡུམ་ གྱི །'.replace(
    #     ' ', '')
    # t = Text(origin)
    # print('已分词原文件：')
    # print(origin)
    # # print(t.tokenize_chunks_plaintext)
    # # print(t.tokenize_on_spaces)
    # a = t.tokenize_words_raw_lines
    # print('基于已分词原文件分词后：')
    # print(a.replace('_', ''))
    # # from pathlib import Path
    # x = Text(in_str)
    # print('未分词原文件：')
    # print(in_str)
    # print('基于未分词原文件分词后：')
    # print(x.tokenize_words_raw_lines)
    # print(tokenize_sentence('''༄༅། །སྙན་ངག་གི་སྡོམ་ཚིག་འཕྲུལ་གྱི་ལྡེ་མིགདུང་དཀར་བློ་བཟང་འཕྲིན་ལས།སྐབས་འདིར་སྙན་ངག་གི་སྡོམ་ཚིག་འདིའི་ལོ་རྒྱུས་ཅུང་ཟད་མཚམས་སྦྱོར་ཞུས་ན། དེ་ཡང་རིག་གནས་ཆུང་བ་ལྔའི་ཡ་གྱལ་སྙན་ཚིག་རིག་པ་འདི་ཉིད་མཁས་པའི་བྱ་བ་གསུམ་གྱི་ནང་ཚན་རྩོམ་རིག་གི་གནད་རྣམས་ལེགས་པར་སྟོན་པ་ཞིག་ཡིན་སྟབས་སྔོན་ནས་ད་ལྟའི་བར་བློ་གསལ་མཁས་དབང་དོན་གཉེར་ཅན་དུ་མས་སློབ་སྦྱོང་མཛད་བཞིན་པར་མ་ཟད། ༼སྙན་ངག་མེ་ལོང་མའི༽རྩ་བ་ཧ་ཅང་རྒྱས་པས་བློ་ལ་ཟིན་དཀའ་བ་དང་། ཨ་ཀྱཱ་ཡོངས་འཛིན་དང་། དངུལ་ཆུ་དྷརྨ་བྷ་དྲའི་སྡོམ་ཚིག་གཉིས་བསྡུས་དྲགས་པ་བཅས་གོང་གསལ་སྙན་ངག་མེ་ལོང་གི་སྡོམ་ཚིག་གཉིས་གཙོ་བོར་བཟུང་། དེར་མ་ཚང་བ་མེ་ལོང་མ་རང་ཉིད་ཀྱི་རྩ་ཚིག་གལ་ཆེ་བ་རྣམས་དང་། ས་པཎ་གྱི་གསུང༼མཁས་པ་ལ་འཇུག་པའི་སྒོ༽གཉིས་ནས་ལེགས་པའི་ཆ་རྣམས་བླངས། དེའི་སྟེང་དུ་གོ་བདེ་བའི་ཚིགས་བཅད་གསར་རྩོམ་འགའ་ཞིག་ཀྱང་སྐབས་སྐབས་སུ་བསྣན་ཏེ། བློ་གྲོས་ཤེས་ཡོན་དམན་རུང་གཟིག་ལྤགས་གྱོན་པའི་བོང་བུའི་ངང་ཚུལ་ལྟར། ཀྲུང་དབྱང་མི་རིགས་སློབ་གྲྭའི་བོད་ཡིག་སློབ་དཔྱོད་དགེ་རྒན་གྱི་མིང་ཙམ་འཛིན་པ་དུང་དཀར་བློ་བཟང་འཕྲིན་ལས་ཀྱིས་རང་ལོ་སོ་ལྔ་པ་སྤྱི་ལོ ༡༩༦༡ ལོར་ཕྱོགས་གཅིག་ཏུ་བསྒྲིགས་པ་ལས། སླར་ཡང་རང་ལོ་ང་བཞི་པ་སྤྱི་ལོ་༡༩༨༠ ལོར་ཉེར་མཁོའི་ཚིགས་བཅད་མང་ཙམ་ཁ་སྣོན་དང་བཟོ་བཅོས་བྱས་པ་ཡིན། །༈ཕྱི་ནང་རིག་གནས་མཐར་སོན་ཅིང་། །ཕུལ་བྱུང་སྙན་ཚིག་འཕྲུལ་ཞགས་ཀྱིས། །རྒྱ་བོད་འཛིན་མ་ཀུན་འཆིང་བ། །སྔོན་བྱོན་མཁས་པའི་དྲིན་གཟོའི་ཆེད། །སྙན་ངག་བསྟན་བཅོས་མེ་ལོང་གི །ལུས་དང་རྒྱན་དང་སྐྱོན་སེལ་གསུམ། །གནད་དོན་སྙིང་པོ་ཡོངས་བསྡུས་པའི།  །སྡོམ་ཚིག་ཅུང་ཟད་དགོད་པར་བྱ། །'''))
    # print(tokenize_sentence('''abc de །།།།  12345།     །qwer།89080།  །'''))
    import re
    para = 'abc་ df་ wang། bi lin'
    print(re.sub('([^།་])([ ])', r"\1་\2", para))