from passlib.context import CryptContext
import bcrypt
from botok import Text
import hashlib

pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")


def verify_password(plain_password, hashed_password):
    return pwd_context.verify(plain_password, hashed_password)


def generate_salt():
    return bcrypt.gensalt().decode()


def get_password_hash(password):
    return pwd_context.hash(password)


def contenttomd5(origin: bytes):
    md5 = hashlib.md5()
    md5.update(origin)
    return md5.hexdigest()


def tokenize_words(content: str):
    a = ''
    try:
        t = Text(content)
        a = t.tokenize_words_raw_lines
        a = a.replace('_', '')
    except:
        pass
    finally:
        return a


if __name__ == '__main__':
    origin = 'p1 ༄ ༅ །  འདུལ་བ་ ཀ་བཞུགས་ སོ །  p2 ln1 ༄ ༅ ༅ །  རྒྱ་གར་ སྐད་ དུ །  བི་ན་ཡ་བསྟུ །  བོད་ སྐད་ དུ །  འདུལ་བ་གཞི །  བམ་པོ་ དང་པོ །  དཀོན་མཆོག་ གསུམ་ ལ་ ཕྱག་ འཚལ་ ལོ །  གང་ གིས་ འཆིང་ ln2 རྣམས་ ཡང་དག་རབ་བཅད་ ཅིང་ །  མུ་སྟེགས་ཚོགས་ རྣམས་ ཐམས་ཅད་ རབ་ བཅོམ་ སྟེ །  སྡེ་དང་ བཅས་པ འི་ བདུད་ རྣམས་ ངེས་བཅོམ་ ནས །  བྱང་ཆུབ་ འདི་ བརྙེས་ དེ་ ལ་ ln3 ཕྱག་ འཚལ་ ལོ །  ཁྱིམ་དོན་ ཆེ་ཆུང་ སྤངས་ ཏེ་ དང་པོ ར་ རབ་ འབྱུང་ དཀའ །  རབ་བྱུང་ ཐོབ་ ནས་ ཡུལ་སྤྱད་ དག་ གིས་ དགའ་ ཐོབ་ དཀའ །  མངོན་ དགའ་ ཇི་ བཞིན་ དོན་ བསྐྱེད་ ཡང་ ln4 དག་ བྱེད་པ་ དཀའ །  ངུར་སྨྲིག་ གོས་ འཆང་མཁས་པ་ ཚུལ་ལས་ ཉམས་པ་ དཀའ །  གཞི་ རྣམས་ ཀྱི་ སྤྱི་སྡོམ་ ལ །  རབ་ འབྱུང་ གསོ་སྦྱོང་ གཞི་ དང་ ནི །  དགག་དབྱེ་ དབྱར་ དང་ ཀོ་ ln5 ལྤགས་གཞི །  སྨན་ དང་ གོས་ དང་ སྲ་བརྐྱང་ དང་ །  ཀཽ་ཤཱམ་བཱི་ དང་ ལས་ ཀྱི་ གཞི །  དམར་སེར་ཅན་ དང་ གང་ཟག་ དང་ །  སྤོ་ དང་ གསོ་སྦྱོང་ བཞག་པ་ དང་ །  གནས་ མལ་ དང་ ནི་ p3 ln1 རྩོད་པ་ དང་ །  དགེ་འདུན་ དབྱེན་ རྣམས་ བསྡུས་པ་ ཡིན །  རབ་ ཏུ་ འབྱུང་བ འི་ གཞི འི་ སྤྱི་ སྡོམ་ ལ །  ཤཱ་རི འི་ བུ་ དང་ མུ་སྟེགས་ཅན །  དགེ་ཚུལ་ གཉིས་ དང་ བྱ་རོག་ སྐྲོད །  དགྲ་བཅོམ་བསད་ དང་ ལག་རྡུམ་ གྱི །'
    in_str = 'p1 ༄ ༅ །  འདུལ་བ་ ཀ་བཞུགས་ སོ །  p2 ln1 ༄ ༅ ༅ །  རྒྱ་གར་ སྐད་ དུ །  བི་ན་ཡ་བསྟུ །  བོད་ སྐད་ དུ །  འདུལ་བ་གཞི །  བམ་པོ་ དང་པོ །  དཀོན་མཆོག་ གསུམ་ ལ་ ཕྱག་ འཚལ་ ལོ །  གང་ གིས་ འཆིང་ ln2 རྣམས་ ཡང་དག་རབ་བཅད་ ཅིང་ །  མུ་སྟེགས་ཚོགས་ རྣམས་ ཐམས་ཅད་ རབ་ བཅོམ་ སྟེ །  སྡེ་དང་ བཅས་པ འི་ བདུད་ རྣམས་ ངེས་བཅོམ་ ནས །  བྱང་ཆུབ་ འདི་ བརྙེས་ དེ་ ལ་ ln3 ཕྱག་ འཚལ་ ལོ །  ཁྱིམ་དོན་ ཆེ་ཆུང་ སྤངས་ ཏེ་ དང་པོ ར་ རབ་ འབྱུང་ དཀའ །  རབ་བྱུང་ ཐོབ་ ནས་ ཡུལ་སྤྱད་ དག་ གིས་ དགའ་ ཐོབ་ དཀའ །  མངོན་ དགའ་ ཇི་ བཞིན་ དོན་ བསྐྱེད་ ཡང་ ln4 དག་ བྱེད་པ་ དཀའ །  ངུར་སྨྲིག་ གོས་ འཆང་མཁས་པ་ ཚུལ་ལས་ ཉམས་པ་ དཀའ །  གཞི་ རྣམས་ ཀྱི་ སྤྱི་སྡོམ་ ལ །  རབ་ འབྱུང་ གསོ་སྦྱོང་ གཞི་ དང་ ནི །  དགག་དབྱེ་ དབྱར་ དང་ ཀོ་ ln5 ལྤགས་གཞི །  སྨན་ དང་ གོས་ དང་ སྲ་བརྐྱང་ དང་ །  ཀཽ་ཤཱམ་བཱི་ དང་ ལས་ ཀྱི་ གཞི །  དམར་སེར་ཅན་ དང་ གང་ཟག་ དང་ །  སྤོ་ དང་ གསོ་སྦྱོང་ བཞག་པ་ དང་ །  གནས་ མལ་ དང་ ནི་ p3 ln1 རྩོད་པ་ དང་ །  དགེ་འདུན་ དབྱེན་ རྣམས་ བསྡུས་པ་ ཡིན །  རབ་ ཏུ་ འབྱུང་བ འི་ གཞི འི་ སྤྱི་ སྡོམ་ ལ །  ཤཱ་རི འི་ བུ་ དང་ མུ་སྟེགས་ཅན །  དགེ་ཚུལ་ གཉིས་ དང་ བྱ་རོག་ སྐྲོད །  དགྲ་བཅོམ་བསད་ དང་ ལག་རྡུམ་ གྱི །'.replace(
        ' ', '')
    t = Text(origin)
    print('已分词原文件：')
    print(origin)
    # print(t.tokenize_chunks_plaintext)
    # print(t.tokenize_on_spaces)
    a = t.tokenize_words_raw_lines
    print('基于已分词原文件分词后：')
    print(a.replace('_', ''))
    # from pathlib import Path
    x = Text(in_str)
    print('未分词原文件：')
    print(in_str)
    print('基于未分词原文件分词后：')
    print(x.tokenize_words_raw_lines)
